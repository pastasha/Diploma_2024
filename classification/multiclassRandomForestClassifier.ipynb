{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 1: Import libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 2: Import data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "processed_data = os.path.abspath('../datasets/processed_data.csv')\n",
    "\n",
    "# merging two csv files\n",
    "dataframe = pd.read_csv(processed_data)\n",
    "\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 3: Split data</h3>\n",
    "\n",
    "Splitting data into independent and dependent variables involves separating the input features (independent variables) from the target variable (dependent variable). The independent variables are used to predict the value of the dependent variable.\n",
    "\n",
    "The data is then split into a training set and a test set, with the training set used to fit the model and the test set used to evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Independent / Dependent variables</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into dependent/independent variables\n",
    "X = dataframe.iloc[:, :-1].values\n",
    "y = dataframe.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding categorical data e.g. gender as a dummy variable\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:,1] = labelencoder_X.fit_transform(X[:,1])\n",
    "\n",
    "# encoding categorical data e.g. disease outcome as a dummy variable\n",
    "y,class_names = pd.factorize(y)\n",
    "\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Train / Test split</b>\n",
    "\n",
    "The data is usually divided into two parts, with the majority of the data used for training the model and a smaller portion used for testing. We have split the data into 75% for training and 25% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, stratify=y, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 4: Feature scaling</h3>\n",
    "\n",
    "Feature scaling is a method of transforming the values of numeric variables so that they have a common scale as machine learning algorithms are sensitive to the scale of the input features.\n",
    "\n",
    "There are two common methods of feature scaling: normalization and standardization.\n",
    "\n",
    "- Normalization scales the values of the variables so that they fall between 0 and 1. This is done by subtracting the minimum value of the feature and dividing it by the range (max-min).\n",
    "- Standardization transforms the values of the variables so that they have a mean of 0 and a standard deviation of 1. This is done by subtracting the mean and dividing it by the standard deviation.\n",
    "\n",
    "Feature scaling is usually performed before training a model, as it can improve the performance of the model and reduce the time required to train it, and helps to ensure that the algorithm is not biased towards variables with larger values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale dataset\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 5: Train model</h3>\n",
    "\n",
    "Training a machine learning model involves using a training dataset to estimate the parameters of the model. The training process uses a learning algorithm that iteratively updates the model parameters, minimizes a loss function, which measures the difference between the predicted values and the actual values in the training data, and updates the model parameters to improve the accuracy of the model.\n",
    "\n",
    "Passed the X_train and y_train data into the model by classifier.fit to train the model with our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classification\n",
    "classifier = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = dataframe.columns.tolist()\n",
    "del column_names[10:]\n",
    "print(column_names)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "#plot_tree(classifier, feature_names=column_names, class_names=class_names, filled=True, rounded=True)\n",
    "tree.plot_tree(classifier.estimators_[0], feature_names=column_names, class_names=class_names, filled = True);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 6: Predict result / Score model</h3>\n",
    "\n",
    "Once the model is trained, it can be used to make predictions on new data. The prediction is made by starting at the root node of the tree and navigating it based on the values of the attributes of the input sample. The prediction is given by the class label at the leaf node reached.\n",
    "\n",
    "The accuracy of the model can be evaluated on a test set, which was previously held out from the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance on training set\n",
    "y_pred_train = classifier.predict(X_train)\n",
    "print(X_train)\n",
    "print(y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = classifier.predict(X_test)\n",
    "print(X_test)\n",
    "print(y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 7: Evaluate model</h3>\n",
    "\n",
    "Accuracy is a useful metric for assessing the performance of a model, but it can be misleading in some cases. For example, in a highly imbalanced dataset, a model that always predicts the majority class will have high accuracy, even though it may not be performing well. Therefore, it is important to consider other metrics, such as confusion matrix, precision, recall, F1-score, and ROC-AUC, along with accuracy, to get a more complete picture of the performance of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Accuracy</b>\n",
    "\n",
    "Accuracy is a commonly used metric for evaluating the performance of a machine learning model. It measures the proportion of correct predictions made by the model on a given dataset.\n",
    "\n",
    "In a binary classification problem, accuracy is defined as the number of correct predictions divided by the total number of predictions. In a multi-class classification problem, accuracy is the average of the individual class accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Classification report</b>\n",
    "\n",
    "A classification report is a summary of the performance of a classification model. It provides several metrics for evaluating the performance of the model on a classification task, including precision, recall, f1-score, and support.\n",
    "\n",
    "The classification report also provides a weighted average of the individual class scores, which takes into account the imbalance in the distribution of classes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(y_train)\n",
    "print(y_pred_train)\n",
    "\n",
    "print(classification_report(y_train, y_pred_train, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>F1 score</b>\n",
    "\n",
    "F1-score is the harmonic mean of precision and recall. It provides a single score that balances precision and recall. Support is the number of instances of each class in the evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 score\n",
    "print(\"F1 Score: \", f1_score(y_train, y_pred_train, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Confusion matrix</b>\n",
    "\n",
    "A confusion matrix is used to evaluate the performance of a classification model. It summarizes the model’s performance by comparing the actual class labels of the data to the predicted class labels generated by the model.\n",
    "\n",
    "True Positives (TP): Correctly predicted positive instances.\n",
    "False Positives (FP): Incorrectly predicted positive instances.\n",
    "True Negatives (TN): Correctly predicted negative instances.\n",
    "False Negatives (FN): Incorrectly predicted negative instances.\n",
    "\n",
    "It provides a clear and detailed understanding of how well the model is performing and helps to identify areas of improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cf_matrix = confusion_matrix(y_train, y_pred_train)\n",
    "sns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Precision-Recall curve</b>\n",
    "\n",
    "A precision-recall curve is a plot that summarizes the performance of a binary classification model as a trade-off between precision and recall and is useful for evaluating the model’s ability to make accurate positive predictions while finding as many positive instances as possible. Precision and Recall are two common metrics for evaluating the performance of a classification model.\n",
    "\n",
    "Precision is the number of true positive predictions divided by the sum of true positive and false positive predictions. It measures the accuracy of the positive predictions made by the model.\n",
    "\n",
    "Recall is the number of true positive predictions divided by the sum of true positive and false negative predictions. It measures the ability of the model to find all positive instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-class classification\n",
    "# fit model\n",
    "clf = OneVsRestClassifier(LogisticRegression())\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "pred_prob = clf.predict_proba(X_test)\n",
    "\n",
    "category_mapping = {\n",
    "    'a_Good': 'Good',\n",
    "    'b_Moderate': 'Moderate',\n",
    "    'c_Unhealthy_for_Sensitive_Groups': 'USG', \n",
    "    'd_Unhealthy' : 'Unhealthy',\n",
    "    'e_Very_Unhealthy' : 'Very Unhealthy',\n",
    "    'f_Severe' : 'Severe'\n",
    "}\n",
    "\n",
    "n_class = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall Curve\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "thresh = {}\n",
    "\n",
    "for i in range(n_class):\n",
    "    fpr[i], tpr[i], thresh[i] = precision_recall_curve(y_test, pred_prob[:, i], pos_label=i)\n",
    "    plt.plot(fpr[i], tpr[i], lw=2, label='{}'.format(category_mapping[class_names[i]]))\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Multiclass Precision-Recall Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>AUC/ROC curve</b>\n",
    "\n",
    "The Receiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) are commonly used metrics for evaluating the performance of a binary classification model.\n",
    "\n",
    "A ROC curve plots the True Positive Rate (TPR) versus the False Positive Rate (FPR) for different thresholds of the model’s prediction probabilities. The TPR is the number of true positive predictions divided by the number of actual positive instances, while the FPR is the number of false positive predictions divided by the number of actual negative instances.\n",
    "\n",
    "The AUC is the area under the ROC curve and provides a single-number metric that summarizes the performance of the model over the entire range of possible thresholds.\n",
    "\n",
    "A high AUC indicates that the model is able to distinguish positive instances from negative instances well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot AUC/ROC curve\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "thresh = {}\n",
    "\n",
    "for i in range(n_class):\n",
    "    fpr[i], tpr[i], thresh[i] = roc_curve(y_test, pred_prob[:, i], pos_label=i)\n",
    "    plt.plot(fpr[i], tpr[i], lw=2, label='{}'.format(category_mapping[class_names[i]]))\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Multiclass ROC Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the test results\n",
    "y_pred=classifier.predict(X_test)\n",
    "\n",
    "# Classification results on test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n",
    "\n",
    "cm=confusion_matrix(y_test,y_pred)\n",
    "print('Confusion Matrix: \\n', cm)\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 8: Feature scaling</h3>\n",
    "\n",
    "Serialize Decision Tree Model in pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('serialized/randomForestModel.pkl', 'wb') as file:\n",
    "    pickle.dump(classifier, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
